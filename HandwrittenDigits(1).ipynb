{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for Visualization: Handwritten Digits\n",
    "\n",
    "The usefulness of dimensionality reduction may not be entirely apparent in only two dimensions, but it becomes clear when looking at high-dimensional data. To see this, let's take a quick look at the application of PCA to the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape     # check the shape (dimensions) of the data array in the digits dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digits dataset consists of 8 √ó 8‚Äìpixel images, meaning that they are 64-dimensional. To gain some intuition into the relationships between these points, we can use PCA to project them into a more manageable number of dimensions, say two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are intersted in how the dataset look like, you can run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3    # choose which data you want to see, the range is from 0 to 1797\n",
    "print(digits.data[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number corresponds to a grayscale pixel intensity in an 8√ó8 digit image:\n",
    "0 ‚Üí Black (no pixel intensity, background)\n",
    "1-15 ‚Üí Increasing grayscale intensity (shades of gray)\n",
    "16 ‚Üí White (maximum intensity, bright pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = digits.data[index].reshape(8, 8)   # Reshape to original 8x8\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Digit Label: {digits.target[index]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the first two principal components of each point to learn about the data, as seen in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('rainbow', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the components mean: \n",
    "\n",
    "We can go a bit further here, and begin to ask what the reduced dimensions mean. This meaning can be understood in terms of combinations of basis vectors. For example, each image in the training set is defined by a collection of 64 pixel values, which we will call the vector  ùë• :\n",
    "\n",
    "ùë•=[ùë•1,ùë•2,ùë•3‚ãØùë•64] \n",
    "\n",
    "One way we can think about this is in terms of a pixel basis. That is, to construct the image, we multiply each element of the vector by the pixel it describes, and then add the results together to build the image:\n",
    "\n",
    "image(ùë•)=ùë•1‚ãÖ(pixel 1)+ùë•2‚ãÖ(pixel 2)+ùë•3‚ãÖ(pixel 3)‚ãØùë•64‚ãÖ(pixel 64) \n",
    "\n",
    "One way we might imagine reducing the dimensionality of this data is to zero out all but a few of these basis vectors. For example, if we use only the first eight pixels, we get an eight-dimensional projection of the data (the following figure). However, it is not very reflective of the whole image: we've thrown out nearly 90% of the pixels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot_pca_component is a function that you do not have to spend too much time to understand for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_components(x, coefficients=None, mean=0, components=None,\n",
    "                        imshape=(8, 8), n_components=8, fontsize=12,\n",
    "                        show_mean=True):\n",
    "    if coefficients is None:\n",
    "        coefficients = x\n",
    "        \n",
    "    if components is None:\n",
    "        components = np.eye(len(coefficients), len(x))\n",
    "        \n",
    "    mean = np.zeros_like(x) + mean\n",
    "        \n",
    "\n",
    "    fig = plt.figure(figsize=(1.2 * (5 + n_components), 1.2 * 2))\n",
    "    g = plt.GridSpec(2, 4 + bool(show_mean) + n_components, hspace=0.3)\n",
    "\n",
    "    def show(i, j, x, title=None):\n",
    "        ax = fig.add_subplot(g[i, j], xticks=[], yticks=[])\n",
    "        ax.imshow(x.reshape(imshape), interpolation='nearest', cmap='binary')\n",
    "        if title:\n",
    "            ax.set_title(title, fontsize=fontsize)\n",
    "\n",
    "    show(slice(2), slice(2), x, \"True\")\n",
    "    \n",
    "    approx = mean.copy()\n",
    "    \n",
    "    counter = 2\n",
    "    if show_mean:\n",
    "        show(0, 2, np.zeros_like(x) + mean, r'$\\mu$')\n",
    "        show(1, 2, approx, r'$1 \\cdot \\mu$')\n",
    "        counter += 1\n",
    "\n",
    "    for i in range(n_components):\n",
    "        approx = approx + coefficients[i] * components[i]\n",
    "        show(0, i + counter, components[i], r'$c_{0}$'.format(i + 1))\n",
    "        show(1, i + counter, approx,\n",
    "             r\"${0:.2f} \\cdot c_{1}$\".format(coefficients[i], i + 1))\n",
    "        if show_mean or i > 0:\n",
    "            plt.gca().text(0, 1.05, '$+$', ha='right', va='bottom',\n",
    "                           transform=plt.gca().transAxes, fontsize=fontsize)\n",
    "\n",
    "    show(slice(2), slice(-2, None), approx, \"Approx\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "\n",
    "fig = plot_pca_components(digits.data[10],\n",
    "                          show_mean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper row of panels shows the individual pixels, and the lower row shows the cumulative contribution of these pixels to the construction of the image. Using only eight of the pixel-basis components, we can only construct a small portion of the 64-pixel image. Were we to continue this sequence and use all 64 pixels, we would recover the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits PCA Components\n",
    "\n",
    "But the pixel-wise representation is not the only choice of basis. We can also use other basis functions, which each contain some predefined contribution from each pixel, and write something like:\n",
    "\n",
    "ùëñùëöùëéùëîùëí(ùë•)=mean+ùë•1‚ãÖ(basis 1)+ùë•2‚ãÖ(basis 2)+ùë•3‚ãÖ(basis 3)‚ãØ \n",
    "\n",
    "PCA can be thought of as a process of choosing optimal basis functions, such that adding together just the first few of them is enough to suitably reconstruct the bulk of the elements in the dataset. The principal components, which act as the low-dimensional representation of our data, are simply the coefficients that multiply each of the elements in this series. the following figure shows a similar depiction of reconstructing the same digit using the mean plus the first eight PCA basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=8)\n",
    "Xproj = pca.fit_transform(digits.data)\n",
    "sns.set_style('white')\n",
    "fig = plot_pca_components(digits.data[10], Xproj[10],\n",
    "                          pca.mean_, pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the pixel basis, the PCA basis allows us to recover the salient features of the input image with just a mean, plus eight components! The amount of each pixel in each component is the corollary of the orientation of the vector in our two-dimensional example. This is the sense in which PCA provides a low-dimensional representation of the data: it discovers a set of basis functions that are more efficient than the native pixel basis of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Number of Components\n",
    "A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data. This can be determined by looking at the cumulative explained variance ratio as a function of the number of components (see the following figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve quantifies how much of the total, 64-dimensional variance is contained within the first  ùëÅ  components. For example, we see that with the digits data the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.\n",
    "\n",
    "This tells us that our 2-dimensional projection loses a lot of information (as measured by the explained variance) and that we'd need about 20 components to retain 90% of the variance. Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in its features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
